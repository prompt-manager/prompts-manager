from fastapi import APIRouter, HTTPException, Body, Query, Depends
from app.core.evaluators import run_evaluation, get_available_metrics
from app.database import database
from app.models.prompt_model import prompts
from app.models.dataset_model import datasets
from app.models.evaluation_result_model import evaluation_results, evaluation_requests, EvaluationStatus
from sqlalchemy import select, insert, func
from datetime import datetime, timezone
from pydantic import BaseModel
from typing import List, Optional
from app.schemas.response_schema import ResponseSchema
from app.schemas.pagination_schema import PaginationParams, PaginatedResponse
from app.utils.response_utils import create_success_response, create_error_response, create_paginated_response, get_total_count
import uuid
import json

router = APIRouter(prefix="/evaluations", tags=["ğŸ§ª 3. í‰ê°€ ê´€ë¦¬"])

# ìƒˆë¡œìš´ ë…¸ë“œ ê¸°ë°˜ í‰ê°€ ìš”ì²­ ìŠ¤í‚¤ë§ˆ
class NodeEvaluationRequest(BaseModel):
    node_name: str
    version: str = "production"  # "production" or specific version number
    dataset_id: int
    metrics: List[str]
    callback_url: Optional[str] = None

# í‰ê°€ ìš”ì²­ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class EvaluationRequestResponse(BaseModel):
    request_id: str
    node_name: str
    version: str
    dataset_id: int
    metrics: List[str]
    status: str
    created_at: datetime

# ë©”íŠ¸ë¦­ ëª©ë¡ ì¡°íšŒ API
@router.get(
    "/metrics",
    tags=["ğŸ“‹ 4. ì¡°íšŒ ë° ê²€ìƒ‰"],
    summary="ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ë©”íŠ¸ë¦­ ëª©ë¡ ì¡°íšŒ",
    description="ì‹œìŠ¤í…œì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë“  í‰ê°€ ë©”íŠ¸ë¦­ì˜ ëª©ë¡ê³¼ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ë©”íŠ¸ë¦­ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "data": [
                            {
                                "key": "accuracy",
                                "name": "ì •í™•ì„±", 
                                "description": "ì˜ˆìƒ ê²°ê³¼ì™€ ì‹¤ì œ ê²°ê³¼ì˜ ì¼ì¹˜ìœ¨ì„ ì¸¡ì •í•©ë‹ˆë‹¤",
                                "unit": "score"
                            },
                            {
                                "key": "response_time",
                                "name": "ì‘ë‹µì†ë„",
                                "description": "í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„± ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤", 
                                "unit": "score"
                            },
                            {
                                "key": "completeness", 
                                "name": "ì™„ì„±ë„",
                                "description": "ì‘ë‹µì˜ ì™„ì „ì„±ê³¼ í¬ê´„ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤",
                                "unit": "score"
                            }
                        ],
                        "message": "ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤."
                    }
                }
            }
        }
    }
)
async def get_metrics():
    """
    ì‹œìŠ¤í…œì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë“  í‰ê°€ ë©”íŠ¸ë¦­ì˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ê° ë©”íŠ¸ë¦­ì˜ í‚¤, ì´ë¦„, ì„¤ëª…, ë‹¨ìœ„ ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    """
    try:
        metrics = get_available_metrics()
        return ResponseSchema(
            status="success",
            data=metrics,
            message="ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤."
        )
    except Exception as e:
        return create_error_response(f"ë©”íŠ¸ë¦­ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ìƒˆë¡œìš´ ë…¸ë“œ ê¸°ë°˜ í‰ê°€ ìš”ì²­ API
@router.post(
    "/request",
    tags=["ğŸ§ª 3. í‰ê°€ ê´€ë¦¬"],
    summary="ğŸš€ ë…¸ë“œ ê¸°ë°˜ í‰ê°€ ìš”ì²­",
    description="ë…¸ë“œì™€ ë²„ì „ì„ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ ë²„ì „ ë˜ëŠ” íŠ¹ì • ë²„ì „ ì§€ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "í‰ê°€ ìš”ì²­ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "data": {
                            "request_id": "uuid-string",
                            "node_name": "ê²€ìƒ‰ë…¸ë“œ",
                            "version": "production",
                            "dataset_id": 1,
                            "metrics": ["accuracy", "response_time"],
                            "status": "pending",
                            "created_at": "2024-01-01T12:00:00Z"
                        },
                        "message": "í‰ê°€ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."
                    }
                }
            }
        },
        404: {
            "description": "ë…¸ë“œ ë˜ëŠ” ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        }
    }
)
async def request_node_evaluation(request: NodeEvaluationRequest = Body(...)):
    """
    ë…¸ë“œ ê¸°ë°˜ í‰ê°€ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.
    - ë…¸ë“œì˜ í”„ë¡œë•ì…˜ ë²„ì „ ë˜ëŠ” íŠ¹ì • ë²„ì „ì„ í‰ê°€
    - ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì„ ë™ì‹œì— í‰ê°€ ê°€ëŠ¥
    - ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒíƒœ ê´€ë¦¬
    """
    try:
        # 1. ë…¸ë“œì™€ ë²„ì „ ê²€ì¦
        if request.version == "production":
            # í”„ë¡œë•ì…˜ ë²„ì „ ì¡°íšŒ
            prompt_query = select(prompts).where(
                prompts.c.node_name == request.node_name,
                prompts.c.production == True
            )
        else:
            # íŠ¹ì • ë²„ì „ ì¡°íšŒ
            prompt_query = select(prompts).where(
                prompts.c.node_name == request.node_name,
                prompts.c.version == int(request.version)
            )
        
        prompt = await database.fetch_one(prompt_query)
        if not prompt:
            raise HTTPException(
                status_code=404, 
                detail=f"Node '{request.node_name}' with version '{request.version}' not found"
            )

        # 2. ë°ì´í„°ì…‹ ê²€ì¦
        dataset_query = select(datasets).where(datasets.c.id == request.dataset_id)
        dataset = await database.fetch_one(dataset_query)
        if not dataset:
            raise HTTPException(status_code=404, detail="Dataset not found")

        # 3. ë©”íŠ¸ë¦­ ê²€ì¦
        available_metrics = [m["key"] for m in get_available_metrics()]
        invalid_metrics = [m for m in request.metrics if m not in available_metrics]
        if invalid_metrics:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid metrics: {invalid_metrics}. Available: {available_metrics}"
            )

        # 4. í‰ê°€ ìš”ì²­ ìƒì„±
        request_id = str(uuid.uuid4())
        
        evaluation_request_data = {
            "id": request_id,
            "node_name": request.node_name,
            "version": request.version,
            "dataset_id": request.dataset_id,
            "metrics": json.dumps(request.metrics),
            "callback_url": request.callback_url,
            "status": EvaluationStatus.PENDING,
            "created_at": datetime.now(timezone.utc)
        }

        insert_query = insert(evaluation_requests).values(**evaluation_request_data)
        await database.execute(insert_query)

        # 5. ì‘ë‹µ ìƒì„±
        response_data = {
            "request_id": request_id,
            "node_name": request.node_name,
            "version": request.version,
            "dataset_id": request.dataset_id,
            "metrics": request.metrics,
            "status": "pending",
            "created_at": evaluation_request_data["created_at"]
        }

        return ResponseSchema(
            status="success",
            data=response_data,
            message="í‰ê°€ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    except HTTPException:
        raise
    except Exception as e:
        return create_error_response(f"í‰ê°€ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# í‰ê°€ ìš”ì²­ ìŠ¤í‚¤ë§ˆ
class EvaluationRequest(BaseModel):
    metric_name: str
    prompt_id: int
    dataset_id: int

# í‰ê°€ ê²°ê³¼ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
class EvaluationResult(BaseModel):
    metric: str
    prompt_id: int
    dataset_id: int
    score: float

@router.post(
    "/run",
    tags=["ğŸ§ª 3. í‰ê°€ ê´€ë¦¬"],
    summary="ğŸš€ í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥",
    description="í”„ë¡¬í”„íŠ¸ì™€ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.",
)
async def evaluate_prompt(request: EvaluationRequest = Body(...)):
    # í”„ë¡¬í”„íŠ¸ ì¡°íšŒ
    prompt_query = select(prompts).where(prompts.c.id == request.prompt_id)
    prompt = await database.fetch_one(prompt_query)
    if not prompt:
        raise HTTPException(status_code=404, detail="Prompt not found.")

    # ë°ì´í„°ì…‹ ì¡°íšŒ
    dataset_query = select(datasets).where(datasets.c.id == request.dataset_id)
    dataset = await database.fetch_one(dataset_query)
    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found.")

    # í‰ê°€ ë¡œì§ ì‹¤í–‰
    try:
        evaluation_score = run_evaluation(
            request.metric_name,
            prompt_content=prompt.content["system"]["prompt"],  
            dataset_content=dataset.content
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # í‰ê°€ ê²°ê³¼ DBì— ì €ì¥
    query = insert(evaluation_results).values(
        prompt_id=request.prompt_id,
        dataset_id=request.dataset_id,
        metric_name=request.metric_name,
        score=evaluation_score,
        created_at=datetime.now(timezone.utc)
    )
    await database.execute(query)

    return create_success_response(
        EvaluationResult(
            metric=request.metric_name,
            prompt_id=request.prompt_id,
            dataset_id=request.dataset_id,
            score=evaluation_score
        ),
        "í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    )

    

# í‰ê°€ ê²°ê³¼ ì¡°íšŒ ìŠ¤í‚¤ë§ˆ
class EvaluationResultRead(BaseModel):
    id: int
    prompt_id: int
    dataset_id: int
    metric_name: str
    score: float
    created_at: datetime

    class Config:
        from_attributes = True

@router.get(
    "/results",
    tags=["ğŸ“‹ 4. ì¡°íšŒ ë° ê²€ìƒ‰"],
    summary="ğŸ“‹ ëª¨ë“  í‰ê°€ ê²°ê³¼ í˜ì´ì§€ë„¤ì´ì…˜ ì¡°íšŒ",
    description="í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. metric_nameê³¼ prompt_idë¡œ í•„í„°ë§ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "í‰ê°€ ê²°ê³¼ í˜ì´ì§€ë„¤ì´ì…˜ ì¡°íšŒ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "data": {
                            "items": [
                                {
                                    "id": 1,
                                    "prompt_id": 1,
                                    "dataset_id": 1,
                                    "metric_name": "accuracy",
                                    "score": 0.95,
                                    "created_at": 1750064190
                                }
                            ],
                            "page": 1,
                            "size": 10,
                            "total": 25,
                            "total_pages": 3,
                            "has_next": True,
                            "has_prev": False
                        },
                        "message": "í‰ê°€ ê²°ê³¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤."
                    }
                }
            },
        }
    },
)
async def get_evaluation_results_paginated(
    pagination: PaginationParams = Depends(),
    metric_name: str = Query(None, description="í‰ê°€ ì§€í‘œë¡œ í•„í„°ë§"),
    prompt_id: int = Query(None, description="í”„ë¡¬í”„íŠ¸ IDë¡œ í•„í„°ë§")
):
    """
    ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    í‰ê°€ ì§€í‘œë‚˜ í”„ë¡¬í”„íŠ¸ë¡œ í•„í„°ë§ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„±
        base_query = select(evaluation_results)
        
        # í•„í„°ë§ ì ìš©
        if metric_name:
            base_query = base_query.where(evaluation_results.c.metric_name == metric_name)
        if prompt_id:
            base_query = base_query.where(evaluation_results.c.prompt_id == prompt_id)
        
        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        count_query = select(func.count()).select_from(
            base_query.alias()
        )
        total = await database.fetch_one(count_query)
        total_count = total[0] if total else 0
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©ëœ ë°ì´í„° ì¡°íšŒ
        paginated_query = (
            base_query
            .order_by(evaluation_results.c.created_at.desc())
            .limit(pagination.size)
            .offset((pagination.page - 1) * pagination.size)
        )
        
        items = await database.fetch_all(paginated_query)
        
        # í•„í„°ë§ ë©”ì‹œì§€ êµ¬ì„±
        filters = []
        if metric_name:
            filters.append(f"ì§€í‘œ: {metric_name}")
        if prompt_id:
            filters.append(f"í”„ë¡¬í”„íŠ¸ ID: {prompt_id}")
        
        filter_message = f" ({', '.join(filters)})" if filters else ""
        message = f"í‰ê°€ ê²°ê³¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤{filter_message}."
        
        return create_paginated_response(
            items=items,
            page=pagination.page,
            size=pagination.size,
            total=total_count,
            message=message
        )
        
    except Exception as e:
        return create_error_response(f"í‰ê°€ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")