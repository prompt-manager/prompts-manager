from fastapi import (
    APIRouter,
    HTTPException,
    UploadFile,
    File,
    Form,
    Body,
    Query,
    Depends,
)
from typing import List, Optional
from app.database import database
from app.models.dataset_model import datasets
from app.schemas.dataset_schema import DatasetRead, DatasetUpdate
from app.schemas.response_schema import ResponseSchema
from app.schemas.pagination_schema import PaginationParams, PaginatedResponse
from app.utils.response_utils import (
    create_success_response,
    create_error_response,
    create_paginated_response,
    get_total_count,
)
from sqlalchemy import select, insert, delete, update, or_, func
from datetime import datetime, timezone
from fastapi.responses import Response, JSONResponse

router = APIRouter(prefix="/datasets", tags=["ğŸ“Š 2. ë°ì´í„°ì…‹ ê´€ë¦¬"])


# ë°ì´í„°ì…‹ ì—…ë¡œë“œ
@router.post(
    "/",
    tags=["ğŸ“Š 2. ë°ì´í„°ì…‹ ê´€ë¦¬"],
    summary="ğŸ“¤ ë°ì´í„°ì…‹ ì—…ë¡œë“œ (CSV)",
    description="CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.",
)
async def upload_dataset(
    name: str = Form(
        ...,
        description="ë°ì´í„°ì…‹ì˜ ì´ë¦„. ì´ ì´ë¦„ì€ ì‹œìŠ¤í…œì—ì„œ ê³ ìœ í•´ì•¼ í•©ë‹ˆë‹¤.",
        example="dataset1",
    ),
    description: Optional[str] = Form(
        None,
        description="ë°ì´í„°ì…‹ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… (ì„ íƒ ì‚¬í•­).",
        example="ì„¤ëª…1",
    ),
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  CSV í˜•ì‹ì˜ íŒŒì¼."),
):
    """
    ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

    - **name**: ë°ì´í„°ì…‹ì˜ ê³ ìœ í•œ ì´ë¦„.
    - **description**: ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„¤ëª….
    - **file**: `.csv` í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼.

    íŒŒì¼ ë‚´ìš©ì€ UTF-8ë¡œ ì¸ì½”ë”©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´, ìƒì„±ëœ ë°ì´í„°ì…‹ì˜ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì¤‘ë³µ ì´ë¦„ ì²´í¬ ì¶”ê°€
    existing_dataset = await database.fetch_one(
        select(datasets).where(datasets.c.name == name)
    )

    if existing_dataset:
        # ì¤‘ë³µ ì‹œì—ë„ ì„±ê³µ ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ messageë¡œ íŒë‹¨)
        return ResponseSchema(
            status="success",
            data=None,
            message=f"Dataset with name '{name}' already exists.",
        )

    if file.content_type != "text/csv":
        raise HTTPException(status_code=400, detail="CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    content_bytes = await file.read()
    try:
        content_str = content_bytes.decode("utf-8")
    except UnicodeDecodeError:
        raise HTTPException(
            status_code=400,
            detail="íŒŒì¼ ì¸ì½”ë”©ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. UTF-8ë¡œ ì¸ì½”ë”©ëœ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤.",
        )

    now = datetime.now(timezone.utc)

    query = (
        insert(datasets)
        .values(name=name, description=description, content=content_str, created_at=now)
        .returning(datasets)
    )

    created_dataset = await database.fetch_one(query)

    # ì„±ê³µ ì‹œ messageë¥¼ nullë¡œ ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—ëŸ¬ ê°ì§€ìš©)
    if isinstance(created_dataset, dict):
        converted_data = created_dataset
    elif hasattr(created_dataset, "_mapping"):  # Database record
        from app.utils.response_utils import convert_record_to_dict

        converted_data = convert_record_to_dict(created_dataset)
    else:
        converted_data = created_dataset

    return ResponseSchema(
        status="success",
        data=converted_data,
        message=None,  # ì„±ê³µ ì‹œ nullë¡œ ì„¤ì •
    )


# ëª¨ë“  ë°ì´í„°ì…‹ í˜ì´ì§€ë„¤ì´ì…˜ ì¡°íšŒ
@router.get(
    "/",
    tags=["ğŸ“‹ 4. ì¡°íšŒ ë° ê²€ìƒ‰"],
    summary="ğŸ“‹ ëª¨ë“  ë°ì´í„°ì…‹ í˜ì´ì§€ë„¤ì´ì…˜ ì¡°íšŒ",
    description="í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤. searchë¡œ ì´ë¦„/ì„¤ëª… ê²€ìƒ‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ë°ì´í„°ì…‹ í˜ì´ì§€ë„¤ì´ì…˜ ì¡°íšŒ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "data": {
                            "items": [
                                {
                                    "id": 1,
                                    "name": "dataset1",
                                    "description": "ì„¤ëª…",
                                    "created_at": 1750064190,
                                }
                            ],
                            "page": 1,
                            "size": 10,
                            "total": 5,
                            "total_pages": 1,
                            "has_next": False,
                            "has_prev": False,
                        },
                        "message": "ë°ì´í„°ì…‹ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
                    }
                }
            },
        }
    },
)
async def get_datasets_paginated(
    pagination: PaginationParams = Depends(),
    search: str = Query(None, description="ì´ë¦„ì´ë‚˜ ì„¤ëª…ìœ¼ë¡œ ê²€ìƒ‰"),
):
    """
    ëª¨ë“  ë°ì´í„°ì…‹ì„ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    ê²€ìƒ‰ì–´ë¡œ í•„í„°ë§ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        # ê¸°ë³¸ ì¿¼ë¦¬ êµ¬ì„±
        base_query = select(datasets)

        # ê²€ìƒ‰ í•„í„°ë§ ì ìš©
        if search:
            base_query = base_query.where(
                or_(
                    datasets.c.name.ilike(f"%{search}%"),
                    datasets.c.description.ilike(f"%{search}%"),
                )
            )

        # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
        count_query = select(func.count()).select_from(base_query.alias())
        total = await database.fetch_one(count_query)
        total_count = total[0] if total else 0

        # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©ëœ ë°ì´í„° ì¡°íšŒ
        paginated_query = (
            base_query.order_by(datasets.c.created_at.desc())
            .limit(pagination.size)
            .offset((pagination.page - 1) * pagination.size)
        )

        items = await database.fetch_all(paginated_query)

        filter_message = f" (ê²€ìƒ‰: {search})" if search else ""
        message = f"ë°ì´í„°ì…‹ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤{filter_message}."

        return create_paginated_response(
            items=items,
            page=pagination.page,
            size=pagination.size,
            total=total_count,
            message=message,
        )

    except Exception as e:
        return create_error_response(
            f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


# ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ (ë“œë¡­ë‹¤ìš´ìš©)
@router.get(
    "/list",
    tags=["ğŸ“‹ 4. ì¡°íšŒ ë° ê²€ìƒ‰"],
    summary="ğŸ“‹ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ",
    description="ë“œë¡­ë‹¤ìš´ì´ë‚˜ ì„ íƒ ëª©ë¡ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "data": [
                            {"id": 1, "name": "dataset1"},
                            {"id": 2, "name": "dataset2"},
                            {"id": 3, "name": "dataset3"},
                        ],
                        "message": "ë°ì´í„°ì…‹ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
                    }
                }
            },
        }
    },
)
async def get_datasets_list():
    """
    ë“œë¡­ë‹¤ìš´ì´ë‚˜ ì„ íƒ ëª©ë¡ìš© ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    idì™€ nameë§Œ í¬í•¨ëœ ê°€ë²¼ìš´ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ëª¨ë“  ë°ì´í„°ì…‹ì„ ìµœì‹  ìƒì„±ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # idì™€ nameë§Œ ì„ íƒí•˜ì—¬ ì¡°íšŒ
        query = select(datasets.c.id, datasets.c.name).order_by(
            datasets.c.created_at.desc()
        )

        items = await database.fetch_all(query)

        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ìˆ˜ë™ ë³€í™˜
        dataset_list = [{"id": item[0], "name": item[1]} for item in items]

        # ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (ResponseSchema ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        return {
            "status": "success",
            "data": dataset_list,
            "message": "ë°ì´í„°ì…‹ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
        }

    except Exception as e:
        return {
            "status": "error",
            "data": None,
            "message": f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
        }


# ë°ì´í„°ì…‹ ì‚­ì œ
@router.delete(
    "/{dataset_id}",
    tags=["ğŸ“Š 2. ë°ì´í„°ì…‹ ê´€ë¦¬"],
    summary="ğŸ—‘ï¸ íŠ¹ì • ë°ì´í„°ì…‹ ì‚­ì œ",
    description="ë°ì´í„°ì…‹ IDë¡œ íŠ¹ì • ë°ì´í„°ì…‹ì„ ì˜êµ¬ ì‚­ì œí•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë¨",
            "content": {
                "application/json": {
                    "example": {"detail": "Dataset 1 has been deleted."}
                }
            },
        },
        404: {
            "description": "ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ",
            "content": {
                "application/json": {"example": {"detail": "Dataset not found."}}
            },
        },
    },
)
async def delete_dataset(dataset_id: int):
    """
    ì§€ì •ëœ IDì˜ ë°ì´í„°ì…‹ì„ ì‚­ì œí•©ë‹ˆë‹¤.

    - **dataset_id**: ì‚­ì œí•  ë°ì´í„°ì…‹ì˜ ê³ ìœ  ID.

    ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ 404 ì˜¤ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    query = select(datasets).where(datasets.c.id == dataset_id)
    existing_dataset = await database.fetch_one(query)

    if not existing_dataset:
        raise HTTPException(status_code=404, detail="Dataset not found.")

    # ë°ì´í„°ì…‹ ì‚­ì œ
    delete_query = delete(datasets).where(datasets.c.id == dataset_id)
    await database.execute(delete_query)

    return create_success_response(
        {"detail": f"Dataset {dataset_id} has been deleted."},
        "ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
    )


# íŠ¹ì • ë°ì´í„°ì…‹ ì¡°íšŒ
@router.get(
    "/{dataset_id}",
    tags=["ğŸ“Š 2. ë°ì´í„°ì…‹ ê´€ë¦¬"],
    summary="ğŸ” íŠ¹ì • ë°ì´í„°ì…‹ ì¡°íšŒ",
    description="ë°ì´í„°ì…‹ IDë¡œ íŠ¹ì • ë°ì´í„°ì…‹ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
)
async def get_dataset(dataset_id: int):
    """
    ì§€ì •ëœ IDë¥¼ ê°€ì§„ ë°ì´í„°ì…‹ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    - **dataset_id**: ì¡°íšŒí•  ë°ì´í„°ì…‹ì˜ ê³ ìœ  ID.

    ë°ì´í„°ì…‹ ë‚´ìš©(`content`)ì„ í¬í•¨í•œ ëª¨ë“  ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ 404 ì˜¤ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    query = select(datasets).where(datasets.c.id == dataset_id)
    dataset = await database.fetch_one(query)

    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found.")

    return create_success_response(dataset, "ë°ì´í„°ì…‹ì„ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.")


# ë°ì´í„°ì…‹ ìˆ˜ì •
@router.put(
    "/{dataset_id}",
    tags=["ğŸ“Š 2. ë°ì´í„°ì…‹ ê´€ë¦¬"],
    summary="âœï¸ ë°ì´í„°ì…‹ ìˆ˜ì •",
    description="ë°ì´í„°ì…‹ì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ ìˆ˜ì •í•©ë‹ˆë‹¤. CSV ë‚´ìš©ì€ ìˆ˜ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
)
async def update_dataset(
    dataset_id: int,
    dataset_update: DatasetUpdate = Body(
        ...,
        examples={
            "ì´ë¦„ë§Œ ë³€ê²½": {
                "summary": "ì´ë¦„ë§Œ ë³€ê²½í•˜ëŠ” ê²½ìš°",
                "value": {"name": "ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì´ë¦„"},
            },
            "ì„¤ëª…ë§Œ ë³€ê²½": {
                "summary": "ì„¤ëª…ë§Œ ë³€ê²½í•˜ëŠ” ê²½ìš°",
                "value": {"description": "ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ì…‹ ì„¤ëª…ì…ë‹ˆë‹¤."},
            },
            "ì´ë¦„ê³¼ ì„¤ëª… ëª¨ë‘ ë³€ê²½": {
                "summary": "ì´ë¦„ê³¼ ì„¤ëª…ì„ í•¨ê»˜ ë³€ê²½í•˜ëŠ” ê²½ìš°",
                "value": {
                    "name": "ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì´ë¦„",
                    "description": "ì—…ë°ì´íŠ¸ëœ ë°ì´í„°ì…‹ ì„¤ëª…ì…ë‹ˆë‹¤.",
                },
            },
        },
    ),
):
    """
    ì§€ì •ëœ IDì˜ ë°ì´í„°ì…‹ì˜ ì´ë¦„ê³¼ ì„¤ëª…ë§Œ ìˆ˜ì •í•©ë‹ˆë‹¤.

    - **dataset_id**: ìˆ˜ì •í•  ë°ì´í„°ì…‹ì˜ ê³ ìœ  ID.
    - **dataset_update**: ì—…ë°ì´íŠ¸í•  ë‚´ìš©ìœ¼ë¡œ `name`, `description` í•„ë“œë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    CSV íŒŒì¼ ë‚´ìš©ì€ ì´ APIì—ì„œ ìˆ˜ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """

    # ê¸°ì¡´ ë°ì´í„°ì…‹ ì¡°íšŒ
    query = select(datasets).where(datasets.c.id == dataset_id)
    existing_dataset = await database.fetch_one(query)

    if not existing_dataset:
        raise HTTPException(status_code=404, detail="Dataset not found.")

    # ìˆ˜ì • ê°€ëŠ¥í•œ í•„ë“œë§Œ ì¶”ì¶œ (name, description)
    update_data = dataset_update.model_dump(exclude_unset=True)

    # ë§Œì•½ contentë¥¼ ìš”ì²­ì— í¬í•¨í–ˆë‹¤ë©´ ì—ëŸ¬ ì²˜ë¦¬
    if "content" in update_data:
        raise HTTPException(
            status_code=400, detail="Content field cannot be updated via this API."
        )

    if not update_data:
        raise HTTPException(status_code=400, detail="No update fields provided.")

    # ì´ë¦„ ì¤‘ë³µ ì²´í¬ (ì´ë¦„ì´ ë³€ê²½ë˜ëŠ” ê²½ìš°ì—ë§Œ)
    if "name" in update_data and update_data["name"] != existing_dataset.name:
        duplicate_check = await database.fetch_one(
            select(datasets).where(
                datasets.c.name == update_data["name"], datasets.c.id != dataset_id
            )
        )
        if duplicate_check:
            raise HTTPException(
                status_code=400,
                detail=f"Dataset with name '{update_data['name']}' already exists.",
            )

    # updated_at í•„ë“œ ì¶”ê°€
    update_data["updated_at"] = datetime.now(timezone.utc)

    # ì—…ë°ì´íŠ¸ ìˆ˜í–‰
    update_query = (
        update(datasets)
        .where(datasets.c.id == dataset_id)
        .values(**update_data)
        .returning(datasets)
    )

    updated_dataset = await database.fetch_one(update_query)

    return create_success_response(
        updated_dataset, "ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
    )


# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
@router.get(
    "/{dataset_id}/download",
    tags=["ğŸ“Š 2. ë°ì´í„°ì…‹ ê´€ë¦¬"],
    summary="ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ",
    description="ë°ì´í„°ì…‹ì„ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.",
    responses={
        200: {
            "description": "ë°ì´í„°ì…‹ì˜ CSV íŒŒì¼ ë‚´ìš©",
            "content": {
                "text/csv": {
                    "schema": {"type": "string", "format": "binary"},
                    "example": "header1,header2\nvalue1,value2\nvalue3,value4",
                }
            },
        },
        404: {
            "description": "ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ",
            "content": {
                "application/json": {"example": {"detail": "Dataset not found."}}
            },
        },
    },
)
async def download_dataset(dataset_id: int):
    """
    ì§€ì •ëœ IDì˜ ë°ì´í„°ì…‹ì„ `.csv` íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

    - **dataset_id**: ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ì…‹ì˜ ê³ ìœ  ID.

    `Content-Disposition` í—¤ë”ê°€ `attachment`ë¡œ ì„¤ì •ë˜ì–´ ë¸Œë¼ìš°ì €ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
    """
    query = select(datasets).where(datasets.c.id == dataset_id)
    dataset = await database.fetch_one(query)

    if not dataset:
        raise HTTPException(status_code=404, detail="Dataset not found.")

    csv_content = dataset.content
    filename = f"{dataset.name}.csv"

    return Response(
        content=csv_content,
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename={filename}"},
    )


# ë°ì´í„°ì…‹ ê²€ìƒ‰
@router.get(
    "/search/",
    tags=["ğŸ“‹ 4. ì¡°íšŒ ë° ê²€ìƒ‰"],
    summary="ğŸ” ë°ì´í„°ì…‹ ê²€ìƒ‰",
    description="ì´ë¦„ì´ë‚˜ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œë¡œ ë°ì´í„°ì…‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
)
async def search_datasets(query: str = Query(..., description="ê²€ìƒ‰í•  í‚¤ì›Œë“œ")):
    query_stmt = select(datasets).where(
        or_(
            datasets.c.name.ilike(f"%{query}%"),
            datasets.c.description.ilike(f"%{query}%"),
        )
    )

    search_results = await database.fetch_all(query_stmt)

    return create_success_response(
        search_results, "ë°ì´í„°ì…‹ ê²€ìƒ‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    )
